# Read me
## Background Information
The performance of traditional Word2Vec model heavily depends on the quality and quantity of the corpus, which violates the way of human learning. To understand the word meaning, human beings prefer a two-stage learning process. That is reading a Linguist-compiled dictionary as well as doing reading comprehension. These two stages complement each other. Traditional Word2Vec can be taken as an analogy of doing reading comprehension, while the first stage, learning the semantic rules from a language dictionary, such as the knowledge of thesaurus and etymology, is usually ignored by existing methods. In this work, we propose a robust word embedding learning framework by imitating the two-stage human learning process. In particular, we construct a semantic manifold based on the thesaurus and etymology to approximate the first stage. More information is described in [our paper](http://www.sciencedirect.com/science/article/abs/pii/S0950705119306173).

## Code
The "word2vec our model.c" is the main model, the "final999.py" is one specific application of our ideas on the dataset simlex999 dictionary which is one essential part of our contribution, and the "word analogy.py" is built to test the result of our model on the testing dataset ".dat". Moreover, I will post another essential function about etymology after the paper being published.
